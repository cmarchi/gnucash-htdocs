<b>SERVER BACK ONLINE, SOME DATA LOSS</b>
2003-04-30   04:59:36

The GnuCash.org Server is back online, and should now be fully operational.
However, there has been some data loss: if you subscribed (or unsubscribed)
to any mailing list, between 12 December 2002 and 28 April 2003, your 
membership info has been lost.  Furthermore, <b>all</b> configuration
info for the German <tt>gnucash-de</tt> mailing list has been lost.
(My sincerest apologies, Christian).  However, most of the mailing list 
archives should be intact (possibly excepting Jan-April 2003, which might
be damaged).  All web pages should work at least as well as before,
and all FTP site contents have been restored and should be fine.
If you experience problems, please report them to me.
<b>Mail is still not being delivered; investigating</b>
<br></br><br></br>
<b><i>So what happened?</i></b>  
It was the classic server-failure triple-whammy.
This server has RAID disk mirrors to minimize down-time due to a 
failed disk, and is backed up nightly in order to safeguard against 
catastrophic data loss.  Hard-drive status was monitored with 
<tt>smartmontools</tt> and reported regularly with <tt>logcheck</tt>.
So how could this belt-and-suspenders system be down so long, and
result in lost data before its all over?
<br></br><br></br>
Over the last few months, <tt>smartmontools</tt> was reporting occasional
disk status changes,  but none of these seemed to be in the form of warnings,
or had any hint of being dire.  At the same time, there were increasing
numbers of <tt>status error: status=0x58 { DriveReady SeekComplete DataRequest }</tt> 
messages showing up in the system log.  In mid-April, these messages started
showing up at least hourly, and were coupled with the cryptic S.M.A.R.T. messages
(it didn't help that I was running the older, more cryptic <tt>smartsuite</tt>,
not the new, improved <tt>smartmontools</tt>).  Finally, the server locked
up, waiting for a DMA to complete, that never would.  Reboot. Locks up.
Reboot again, locks up (warlord calls by phone to point this out).  
I disabled DMA, went to PIO-mode for the disk in question, and things 
cleared up.  I then made my first thinko:  I concluded that one of my hard
drives was on the verge of failing completely (the DMA should have clue-trained 
me in).  I procured a replacement disk, and then made my second 
'operator error':  I replaced the failing disk.
My logic was this:  there are two disks in the raid array; both are exact
duplicates of each other.  Therefore, if I replace the failed disk, the 
contents of the good disk will be restored onto the blank disk automatically.
Easy as pie.  I've done it many times before.   It didn't work this time.
Upon reboot, I got a gazillion <tt>fsck</tt>'ing errors, the file system was corrupted. 
In addition, I was getting a <i>lot</i> of <tt>status error: status=0x58 
{ DriveReady SeekComplete DataRequest }</tt> from what used to be the 'good' disk.
I plowed on.  At this time, I assumed that maybe both disks were bad, 
a reasonable assumption; these were the infamous IBM-lawsuit drives.
I guessed that the raid array was hiding the badness from me: 
whenever one disk had trouble, the RAID would go to 
the other disk, and all was well in the kingdom, even though anarchy seethed
just below the surface.  Oh well.  I procured a second hard drive, and 
replaced that.  With more <tt>fsck</tt>'ing error in the process.  Then I notice
that I'm still getting SeekComplete's in the syslog, even with the new 
disks. Now, the replacement disks are the same lawsuit-brand and model number 
as the old disks, so woe is me, this is my third mistake, I assume, incorrectly, 
that its the brand and model number, and get a new third disk.  When the errors 
don't abate, it suddenly becomes clear that its not the drives, its the 
controller or cable).
<br></br><br></br>
Now the fun and hacking begin.  Stay up til three AM playing the 
swap-the-cables-and-reboot game.  The failing controller was on the
system planar ('motherboard'), so there is no way to remove it; one
can only play with BIOS settings.   But BIOS (and the Linux kernel 
shares the blame) has this magic way of renumbering IDE drives when
one plugs in or removes controllers, enables or disables controller
ports, etc.  This can be overcome, but is a provides a steady stream
of hurdles to jump: one must boot a rescue diskette first, then
mount, then re-write the boot sector, then reboot, then edit 
<tt>/etc/fstab</tt>, and then try again. Over and over and over.
It didn't help that my rescue diskette didn't have RAID on it:
so that was one more thing to hack around.   Finally build
a stable system, and now it comes time to restore the data
files that were <tt>fsck</tt>'ed out of existence.  To restore 
<tt>/usr</tt>, I decide that reinstall of the OS is appropriate.
I then restore the FTP site, which was badly corrupted.  Restore
the mailing lists; no problems, only October 1998 was lost and 
restored.  Restore the website; only minor damage there. 
Then restore the mailing list subscriber info in 
<tt>/var/lib/mailman/lists</tt> ... Uhh ... whoops.  That directory
was <i>not</i> backed up nightly.   I had falsely assumed that 
everything in <tt>/var/lib/mailman/lists</tt> was stuff that could
be recovered by re-installing <tt>mailman</tt>.  I had no idea that it
kept subscriber info there.  Mistake number four (number zero?):
this critical directory was not one that was backed up nightly.
I was lucky to find a December 2002 backup of it;  it could 
have easily been December 2001 and then I really would have felt 
sorry.  Mistake five: turns out the backup machine had overflowed,
and stopped making backups on 5 April.  Fortunately for me, not
much has happened since 5 April.   I hope this little story makes
it clear that running a public web site in a professional manner
can sometimes be a walk in the park, and sometimes guerilla warfare.
BTW, yes, every now and then, this server is backed up to an
off-site location, so that if the machine is stolen, or the house 
burns down, all is not lost.  
<br></br><br></br>
Also, BTW, my 'failed' disks are fine: once they were off the bad 
controller, the errors stopped. I am still eyeing them with suspicion,
but ... 

<br></br><br></br>
Some lessons drawn from this, or rather some complaints, are 
documented at
<a href="http://www.linas.org/linux/peeves.html">http://www.linas.org/linux/peeves.html</a>

